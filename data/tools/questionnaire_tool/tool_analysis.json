{
  "tool_name": "questionnaire_tool",
  "tool_code": "import json\nimport os\nfrom typing import Dict, List, Any, Optional\n\nfrom tools.repo import Tool\nfrom errors import ErrorCode, error_context, ToolError\nfrom config import config\nfrom api.llm_provider import LLMBridge\n\n\nclass QuestionnaireTool(Tool):\n    \"\"\"\n    Interactive questionnaire tool that collects structured user responses.\n\n    This tool conducts a sequential question-answer session with the user,\n    keeping the interaction local (not sending responses to LLM) until all\n    questions are answered. It can:\n    1. Use simple string questions provided directly in the 'questions' parameter\n    2. Load predefined questionnaires from files using 'questionnaire_id'\n    3. Accept custom structured question objects via 'custom_questions'\n    \n    The flexible interface allows for both quick ad-hoc questioning and more\n    complex structured questionnaires with advanced features.\n    \"\"\"\n\n    name = \"questionnaire_tool\"\n    description = \"\"\"Manages interactive multi-question surveys to collect structured information from users without sending intermediate responses to the LLM.\n\nThis tool enables conducting comprehensive questionnaires with various customization options:\n\n1. Running Questionnaires:\n   - Use predefined questionnaires via 'questionnaire_id' parameter\n   - Create ad-hoc questionnaires with 'questions' parameter (simple string list)\n   - Design custom structured questionnaires with 'custom_questions' parameter\n   - Example: questionnaire_id=\"recipe\" or questions=[\"What's your name?\", \"Where do you live?\"]\n\n2. Question Types and Features:\n   - Simple text questions for free-form responses\n   - Multiple-choice questions with predefined options\n   - Dynamic question generation based on previous answers\n   - Preference key mapping for organizing responses with meaningful labels\n   - Automatic filtering of already-answered questions\n   \n3. Response Handling:\n   - Collects all responses locally without sending to LLM until complete\n   - Returns structured data with question/answer pairs\n   - Maps responses to semantic keys for easier preference management\n   - IMPORTANT: When presenting results, show only the raw data without interpretive commentary\n\nUse this tool whenever you need to gather multiple pieces of structured information from the user in a single interaction session.\"\"\"\n    usage_examples = [\n#         {\n#             \"input\": {\"questionnaire_id\": \"recipe\"},\n#             \"output\": {\n#                 \"questionnaire_id\": \"recipe\",\n#                 \"completed\": True,\n#                 \"responses\": {\n#                     \"Cuisine\": \"Italian\",\n#                     \"Ingredients to use\": \"Tomatoes\",\n#                     \"Dietary restrictions\": \"No\",\n#                     \"Cooking time\": \"Less than 30 minutes\"\n#                 }\n#             },\n#             \"response_example\": \"\"\"\n# ### Questionnaire Results: recipe\n# - Cuisine: Italian\n# - Ingredients to use: Tomatoes\n# - Dietary restrictions: No\n# - Cooking time: Less than 30 minutes\n# \"\"\"\n#         },\n#         {\n#             \"input\": {\"questionnaire_id\": \"quick_survey\", \"questions\": [\"What is your name?\", \"How old are you?\", \"Where do you live?\"]},\n#             \"output\": {\n#                 \"questionnaire_id\": \"quick_survey\",\n#                 \"completed\": True,\n#                 \"responses\": {\n#                     \"q1\": \"John Doe\",\n#                     \"q2\": \"30\",\n#                     \"q3\": \"New York\"\n#                 }\n#             },\n#             \"response_example\": \"\"\"\n# ### Questionnaire Results: quick_survey\n# - What is your name?: John Doe\n# - How old are you?: 30\n# - Where do you live?: New York\n# \"\"\"\n#         }\n    ]\n\n    def __init__(self, llm_provider: LLMBridge):\n        \"\"\"\n        Initialize the questionnaire tool.\n        \n        Args:\n            llm_provider: LLMBridge instance for generating dynamic content\n        \"\"\"\n        super().__init__()\n        # Tool-specific state\n        self.data_dir = config.paths.data_dir\n        self.questionnaire_dir = os.path.join(self.data_dir, \"tools\", \"questionnaire_tool\")\n        self.llm_provider = llm_provider\n        \n        # Ensure directories exist\n        os.makedirs(self.data_dir, exist_ok=True)\n        os.makedirs(self.questionnaire_dir, exist_ok=True)\n\n    def run(\n        self,\n        questionnaire_id: str,\n        custom_questions: Optional[List[Dict[str, Any]]] = None,\n        context_data: Optional[Dict[str, Any]] = None,\n        questions: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Run an interactive questionnaire session with the user.\n\n        This method supports three ways to provide questions, in order of priority:\n        1. Simple questions via 'questions' parameter (highest priority)\n        2. Structured questions via 'custom_questions' parameter\n        3. Questions from a predefined file via 'questionnaire_id' (lowest priority)\n\n        Args:\n            questionnaire_id: ID of the questionnaire to use (e.g., \"recipe\") or a name/description \n                             for a questionnaire created from simple questions\n            custom_questions: Optional list of custom structured question objects\n                             Each object should contain at least 'id' and 'text' keys\n            context_data: Optional contextual data to use for dynamic question generation\n                         and to filter out already answered questions\n            questions: Optional list of question strings or a JSON string representing a list of strings\n                       for on-the-fly questionnaire creation.\n                       Examples:\n                       - List: [\"What is your name?\", \"Where are you going?\"]\n                       - JSON string: \"[\\\"What is your name?\\\", \\\"Where are you going?\\\"]\"\n                       This is the simplest way to use the tool - just provide questions\n                       and the tool will handle converting them to the proper format\n\n        Returns:\n            Dictionary containing questionnaire results including all responses:\n            {\n                \"questionnaire_id\": str,\n                \"completed\": bool,\n                \"responses\": {question_id: response, ...}\n            }\n\n        Raises:\n            ToolError: If questionnaire file not found or has invalid format\n        \"\"\"\n        self.logger.info(f\"Starting questionnaire: {questionnaire_id}\")\n\n        # Use the centralized error context for questionnaire operations\n        with error_context(\n            component_name=self.name,\n            operation=\"running questionnaire\",\n            error_class=ToolError,\n            error_code=ErrorCode.TOOL_EXECUTION_ERROR,\n            logger=self.logger\n        ):\n            # Initialize context if not provided\n            if context_data is None:\n                context_data = {}\n                \n            # Load questions - priority: simple questions > custom_questions > file\n            if questions:\n                # Handle JSON string format\n                if isinstance(questions, str):\n                    try:\n                        import json\n                        parsed = json.loads(questions)\n                        \n                        if not isinstance(parsed, list):\n                            raise ToolError(\n                                \"Questions parameter as JSON must represent a list of strings\",\n                                ErrorCode.TOOL_INVALID_INPUT,\n                                {\"parsed_type\": type(parsed).__name__}\n                            )\n                        \n                        # Replace with parsed list\n                        self.logger.info(\"Converted questions from JSON string to list\")\n                        questions = parsed\n                    except json.JSONDecodeError as e:\n                        raise ToolError(\n                            f\"Failed to parse questions parameter as JSON: {str(e)}\",\n                            ErrorCode.TOOL_INVALID_INPUT,\n                            {\"error\": str(e)}\n                        )\n                \n                # At this point, questions should be a list (either originally or after parsing)\n                if not isinstance(questions, list):\n                    raise ToolError(\n                        \"The 'questions' parameter must be a list of strings or a JSON string representing a list\",\n                        ErrorCode.TOOL_INVALID_INPUT,\n                        {\"actual_type\": type(questions).__name__}\n                    )\n                \n                # Validate all items are strings\n                for i, q in enumerate(questions):\n                    if not isinstance(q, str):\n                        raise ToolError(\n                            f\"Each question must be a string. Question {i+1} is {type(q).__name__}\",\n                            ErrorCode.TOOL_INVALID_INPUT,\n                            {\"position\": i, \"type\": type(q).__name__}\n                        )\n                \n                # Convert to question objects\n                questions = self._convert_simple_questions(questions)\n                self.logger.debug(f\"Created questionnaire from {len(questions)} simple questions\")\n            elif custom_questions and not isinstance(custom_questions, str):\n                # Use provided custom questions\n                questions = custom_questions\n                self.logger.debug(f\"Using custom questions for questionnaire {questionnaire_id}\")\n            else:\n                # Load from file - handle natural language requests using LLM\n                questions = self._load_questionnaire_file(questionnaire_id)\n                \n            # Process dynamic questions if needed\n            if self.llm_provider:\n                questions = self._process_dynamic_questions(questions, context_data)\n                \n            # Filter out questions that have already been answered\n            if context_data and self.llm_provider:\n                original_count = len(questions)\n                questions = self.filter_answered_questions(questions, context_data)\n                filtered_count = original_count - len(questions)\n                if filtered_count > 0:\n                    self.logger.info(f\"Filtered out {filtered_count} already answered questions\")\n\n            # Run the interactive questionnaire\n            responses = self._run_interactive_questionnaire(questions, context_data)\n            \n            # Format and return the results\n            result = {\n                \"questionnaire_id\": questionnaire_id,\n                \"completed\": True,\n                \"responses\": responses\n            }\n            \n            self.logger.info(f\"Completed questionnaire: {questionnaire_id}\")\n            self.logger.debug(f\"Questionnaire responses: {responses}\")\n            return result\n            \n    def _load_questionnaire_file(self, questionnaire_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load a questionnaire file, with support for natural language mapping.\n        \n        Args:\n            questionnaire_id: The ID or natural language description of the questionnaire\n            \n        Returns:\n            List of question dictionaries\n            \n        Raises:\n            ToolError: If questionnaire file is not found or has invalid format\n        \"\"\"\n        # Clean up the questionnaire ID\n        questionnaire_id_clean = questionnaire_id.replace('.json', '').lower()\n        \n        # Check if this is a natural language request\n        if self.llm_provider and len(questionnaire_id_clean.split()) > 1:\n            # Try to match the natural language request to a questionnaire\n            try:\n                # Get available questionnaires\n                available_files = os.listdir(self.questionnaire_dir)\n                questionnaire_files = [f for f in available_files if f.endswith('_questionnaire.json')]\n                \n                if questionnaire_files:\n                    # Extract topics\n                    questionnaire_topics = []\n                    for f in questionnaire_files:\n                        topic = f.replace('_questionnaire.json', '')\n                        questionnaire_topics.append(topic)\n                    \n                    # Ask LLM to match the request to a questionnaire\n                    prompt = f\"\"\"Based on this request: \"{questionnaire_id}\", \nwhich of these questionnaire topics is the most appropriate match?\nAvailable topics: {', '.join(questionnaire_topics)}\n\nReturn only the exact name of the best matching topic from the list.\"\"\"\n                    \n                    messages = [{\"role\": \"user\", \"content\": prompt}]\n                    response = self.llm_provider.generate_response(messages)\n                    matched_topic = self.llm_provider.extract_text_content(response).strip().lower()\n                    \n                    self.logger.info(f\"LLM matched request '{questionnaire_id}' to topic: {matched_topic}\")\n                    questionnaire_id_clean = matched_topic\n            except Exception as e:\n                self.logger.error(f\"Error during LLM topic matching: {e}\")\n        \n        # Construct the questionnaire file path\n        questionnaire_filename = f\"{questionnaire_id_clean}_questionnaire.json\"\n        questionnaire_path = os.path.join(self.questionnaire_dir, questionnaire_filename)\n        \n        self.logger.info(f\"Looking for questionnaire file: {questionnaire_path}\")\n        \n        # Check if the file exists\n        if not os.path.exists(questionnaire_path):\n            # List available questionnaires\n            try:\n                available_files = os.listdir(self.questionnaire_dir)\n                questionnaire_topics = []\n                for f in available_files:\n                    if f.endswith('_questionnaire.json'):\n                        topic = f.replace('_questionnaire.json', '')\n                        questionnaire_topics.append(topic)\n                \n                self.logger.info(f\"Available questionnaires: {questionnaire_topics}\")\n                \n                # Return a helpful error message with available options\n                available_msg = \", \".join(questionnaire_topics) if questionnaire_topics else \"none found\"\n                error_message = (\n                    f\"Questionnaire not found for: '{questionnaire_id_clean}'. \"\n                    f\"Available questionnaires: {available_msg}. \"\n                    f\"Please try again with one of these topics.\"\n                )\n                \n                raise ToolError(\n                    error_message,\n                    ErrorCode.TOOL_INVALID_INPUT,\n                    {\n                        \"questionnaire_id\": questionnaire_id_clean,\n                        \"available_questionnaires\": questionnaire_topics\n                    }\n                )\n            except Exception as e:\n                if isinstance(e, ToolError):\n                    raise\n                \n                self.logger.error(f\"Error listing available questionnaires: {e}\")\n                raise ToolError(\n                    f\"Questionnaire not found: {questionnaire_id_clean}\",\n                    ErrorCode.TOOL_INVALID_INPUT,\n                    {\"questionnaire_id\": questionnaire_id_clean}\n                )\n        \n        # Read and parse the questionnaire file\n        try:\n            with open(questionnaire_path, 'r') as f:\n                questionnaire_data = json.load(f)\n                \n            # Validate questionnaire format\n            if not isinstance(questionnaire_data, list) or not questionnaire_data:\n                raise ToolError(\n                    f\"Invalid questionnaire format for {questionnaire_id}\",\n                    ErrorCode.TOOL_INVALID_INPUT\n                )\n            \n            self.logger.debug(f\"Loaded {len(questionnaire_data)} questions for questionnaire {questionnaire_id}\")\n            return questionnaire_data\n            \n        except json.JSONDecodeError:\n            raise ToolError(\n                f\"Invalid JSON in questionnaire file: {questionnaire_id}\",\n                ErrorCode.TOOL_INVALID_INPUT\n            )\n    \n    def _convert_simple_questions(self, question_strings: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Convert a list of simple question strings into properly formatted question objects.\n        \n        This method treats all simple questions as free-text questions, allowing users\n        to provide natural language responses without predefined options.\n        \n        Args:\n            question_strings: List of question strings\n            \n        Returns:\n            List of properly formatted question objects\n        \"\"\"\n        questions = []\n        \n        for i, question_text in enumerate(question_strings):\n            # Clean up the question text\n            question_text = question_text.strip()\n            if not question_text:\n                continue\n                \n            # Create a unique ID for the question\n            question_id = f\"q{i+1}\"\n            \n            # Create the question object as a free-text question\n            # No options are provided to allow for natural language responses\n            question = {\n                \"id\": question_id,\n                \"text\": question_text\n            }\n            \n            questions.append(question)\n            \n        self.logger.debug(f\"Converted {len(questions)} simple questions to question objects\")\n        return questions\n        \n    def filter_answered_questions(\n        self, \n        questions: List[Dict[str, Any]], \n        context_data: Dict[str, Any]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Filter out questions that have already been answered based on context data.\n        \n        Uses LLM to determine if a question has already been answered.\n        \n        Args:\n            questions: List of question objects\n            context_data: Contextual data containing potential answers\n            \n        Returns:\n            List of questions that still need to be answered\n        \"\"\"\n        if not context_data or not self.llm_provider:\n            return questions\n            \n        # Create a combined context string\n        context_str = \"\\n\".join([f\"{k}: {v}\" for k, v in context_data.items()])\n        \n        filtered_questions = []\n        for question in questions:\n            question_text = question.get(\"text\", \"\")\n            \n            if not question_text:\n                continue\n                \n            # Ask LLM if this question is already answered in the context\n            prompt = f\"\"\"\nContext Information:\n{context_str}\n\nQuestion: {question_text}\n\nBased ONLY on the context information provided above, is this question already answered?\nAnswer with ONLY \"yes\" or \"no\".\n\"\"\"\n            \n            try:\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                response = self.llm_provider.generate_response(messages)\n                content = self.llm_provider.extract_text_content(response).strip().lower()\n                \n                # If the answer is not clearly \"yes\", include the question\n                if content != \"yes\":\n                    filtered_questions.append(question)\n                else:\n                    self.logger.debug(f\"Filtered out already answered question: {question_text}\")\n            except Exception as e:\n                self.logger.error(f\"Error checking if question is answered: {e}\")\n                # Include the question if there's an error\n                filtered_questions.append(question)\n        \n        return filtered_questions\n        \n    def _process_dynamic_questions(\n        self, \n        questions: List[Dict[str, Any]], \n        context_data: Dict[str, Any]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Process questions with dynamic content from LLM.\n        \n        This handles questions that need personalization based on previous answers\n        or other contextual data.\n        \n        Args:\n            questions: List of question objects\n            context_data: Contextual data including previous answers\n            \n        Returns:\n            Updated list of questions with dynamic content resolved\n        \"\"\"\n        processed_questions = []\n        \n        for question in questions:\n            # Make a copy to avoid modifying the original\n            q = question.copy()\n            \n            # Check if this is a dynamic question\n            if q.get('dynamic_type') == 'llm_options':\n                # Generate options using LLM\n                prompt_template = q.get('prompt_template', '')\n                \n                # Replace placeholders in the prompt template\n                prompt = prompt_template\n                for key, value in context_data.items():\n                    placeholder = f\"{{{{{key}}}}}\"\n                    if placeholder in prompt:\n                        prompt = prompt.replace(placeholder, str(value))\n                \n                # Get options from LLM\n                if prompt and self.llm_provider:\n                    self.logger.debug(f\"Generating dynamic options with prompt: {prompt}\")\n                    try:\n                        # Create message format for the bridge\n                        messages = [\n                            {\"role\": \"user\", \"content\": prompt}\n                        ]\n                        \n                        # Call the LLM to generate options\n                        response = self.llm_provider.generate_response(messages)\n                        content = self.llm_provider.extract_text_content(response)\n                        \n                        # Parse options (assuming one per line)\n                        options = [line.strip() for line in content.strip().split('\\n') if line.strip()]\n                        q['options'] = options\n                        self.logger.debug(f\"Generated options: {options}\")\n                    except Exception as e:\n                        self.logger.error(f\"Error generating dynamic options: {e}\")\n                        # Fallback options\n                        q['options'] = q.get('fallback_options', ['Option not available'])\n            \n            processed_questions.append(q)\n            \n        return processed_questions\n\n    def _run_interactive_questionnaire(\n        self, \n        questions: List[Dict[str, Any]], \n        context_data: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Runs the interactive questionnaire with the user.\n        \n        This method conducts the local interaction loop with the user,\n        presenting questions and collecting responses without sending\n        intermediate responses to the LLM.\n        \n        Args:\n            questions: List of question objects, each containing at minimum\n                      'id', 'text', and either 'options' or 'free_text' flag\n            context_data: Optional contextual data for dynamic content and\n                         updating context between questions\n                      \n        Returns:\n            Dictionary of responses mapped to question keys\n        \"\"\"\n        # Initialize responses dictionary \n        responses = {}\n        \n        # Create a working copy of context data that will be updated as we go\n        working_context = context_data.copy() if context_data else {}\n        \n        # Start questionnaire message\n        print(\"\\n--- Starting Interactive Questionnaire ---\")\n        print(\"Please answer the following questions. Your answers will be collected locally.\")\n        print(\"Type your answer or select from the options provided.\\n\")\n        \n        # Process each question\n        for i, question in enumerate(questions):\n            # Extract question details\n            question_id = question.get('id', f\"q{i+1}\")\n            question_text = question.get('text', \"\")\n            options = question.get('options', [])\n            \n            # Display question\n            print(f\"\\nQ{i+1}: {question_text}\")\n            \n            # If multiple-choice question, display options\n            if options:\n                for j, option in enumerate(options):\n                    print(f\"  {j+1}. {option}\")\n                \n                # Get and validate user response\n                while True:\n                    user_input = input(\"\\nYour answer (enter number or text): \").strip()\n                    \n                    # Check if user entered a number\n                    try:\n                        option_num = int(user_input)\n                        if 1 <= option_num <= len(options):\n                            response = options[option_num-1]\n                            break\n                        else:\n                            print(f\"Please enter a number between 1 and {len(options)}\")\n                    except ValueError:\n                        # User entered text, check if it matches an option\n                        if user_input in options:\n                            response = user_input\n                            break\n                        else:\n                            # Always accept free text (implicit allow_free_text for all questions)\n                            response = user_input\n                            break\n            else:\n                # Free text question\n                response = input(\"\\nYour answer: \").strip()\n            \n            # Store the response\n            responses[question_id] = response\n            print(f\"Response recorded: {response}\")\n            \n            # Update working context with this response\n            working_context[question_id] = response\n            \n            # If there are dependent questions coming up, generate their content now\n            if i < len(questions) - 1:\n                next_questions = questions[i+1:]\n                dynamic_next = [q for q in next_questions if q.get('dynamic_type') == 'llm_options']\n                \n                if dynamic_next and self.llm_provider:\n                    # Process the next dynamic questions with updated context\n                    questions[i+1:] = self._process_dynamic_questions(next_questions, working_context)\n        \n        # End questionnaire message\n        print(\"\\n--- Questionnaire Complete ---\")\n        print(\"Thank you for your responses. Returning to the conversation.\\n\")\n        \n        # Map question IDs to any provided preference keys\n        preference_mapping = {}\n        for question in questions:\n            qid = question.get('id')\n            pref_key = question.get('preference_key', qid)\n            if qid and pref_key != qid:\n                preference_mapping[qid] = pref_key\n        \n        # Apply preference mapping to responses if specified\n        if preference_mapping:\n            mapped_responses = {}\n            for qid, response in responses.items():\n                mapped_key = preference_mapping.get(qid, qid)\n                mapped_responses[mapped_key] = response\n            return mapped_responses\n        \n        return responses",
  "description": "An interactive questionnaire tool that conducts sequential question-answer sessions with users, collecting structured responses locally without sending intermediate responses to the LLM until all questions are answered.",
  "operations": [
    {
      "name": "run",
      "description": "Runs an interactive questionnaire session with the user, supporting three ways to provide questions: simple string questions, structured custom questions, or predefined questionnaires from files.",
      "required_parameters": [
        {
          "name": "questionnaire_id",
          "type": "str",
          "description": "ID of the questionnaire to use or a name/description for a questionnaire created from simple questions"
        }
      ],
      "optional_parameters": [
        {
          "name": "custom_questions",
          "type": "Optional[List[Dict[str, Any]]]",
          "description": "List of custom structured question objects, each containing at least 'id' and 'text' keys",
          "default": "None"
        },
        {
          "name": "context_data",
          "type": "Optional[Dict[str, Any]]",
          "description": "Contextual data to use for dynamic question generation and to filter out already answered questions",
          "default": "None"
        },
        {
          "name": "questions",
          "type": "Optional[List[str]]",
          "description": "List of question strings or a JSON string representing a list of strings for on-the-fly questionnaire creation",
          "default": "None"
        }
      ]
    },
    {
      "name": "_load_questionnaire_file",
      "description": "Loads a questionnaire file from disk, with support for natural language mapping to find the appropriate questionnaire.",
      "required_parameters": [
        {
          "name": "questionnaire_id",
          "type": "str",
          "description": "The ID or natural language description of the questionnaire to load"
        }
      ],
      "optional_parameters": []
    },
    {
      "name": "_convert_simple_questions",
      "description": "Converts a list of simple question strings into properly formatted question objects with unique IDs.",
      "required_parameters": [
        {
          "name": "question_strings",
          "type": "List[str]",
          "description": "List of question strings to convert to question objects"
        }
      ],
      "optional_parameters": []
    },
    {
      "name": "filter_answered_questions",
      "description": "Filters out questions that have already been answered based on context data, using LLM to determine if a question has been answered.",
      "required_parameters": [
        {
          "name": "questions",
          "type": "List[Dict[str, Any]]",
          "description": "List of question objects to filter"
        },
        {
          "name": "context_data",
          "type": "Dict[str, Any]",
          "description": "Contextual data containing potential answers"
        }
      ],
      "optional_parameters": []
    },
    {
      "name": "_process_dynamic_questions",
      "description": "Processes questions with dynamic content, generating options using LLM based on previous answers or contextual data.",
      "required_parameters": [
        {
          "name": "questions",
          "type": "List[Dict[str, Any]]",
          "description": "List of question objects to process"
        },
        {
          "name": "context_data",
          "type": "Dict[str, Any]",
          "description": "Contextual data including previous answers"
        }
      ],
      "optional_parameters": []
    },
    {
      "name": "_run_interactive_questionnaire",
      "description": "Conducts the local interaction loop with the user, presenting questions and collecting responses without sending intermediate responses to the LLM.",
      "required_parameters": [
        {
          "name": "questions",
          "type": "List[Dict[str, Any]]",
          "description": "List of question objects, each containing at minimum 'id', 'text', and either 'options' or 'free_text' flag"
        }
      ],
      "optional_parameters": [
        {
          "name": "context_data",
          "type": "Optional[Dict[str, Any]]",
          "description": "Contextual data for dynamic content and updating context between questions",
          "default": "None"
        }
      ]
    }
  ],
  "complexity_category": "standard",
  "recommended_examples": 45
}